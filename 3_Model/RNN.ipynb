{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature columns used in training: ['KiWo', 'Is_Ferien', 'Holiday', 'Is_Weekend', 'Weihnachten_Sommer', 'Christmas_Sales', 'Temperature_Category', 'Windgeschwindigkeit_Beaufort', 'Rain_Status', 'Cloud_Status', 'Warengruppe_1', 'Warengruppe_2', 'Warengruppe_3', 'Warengruppe_4', 'Warengruppe_5', 'Warengruppe_6', 'Umsatz_7day_avg', 'Umsatz_last_year']\n",
      "Warning: Could not process column Umsatz_last_year - arg must be a list, tuple, 1-d array, or Series\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41042/2445998602.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[col] = pd.to_numeric(train_data[col], errors='coerce').fillna(0)\n",
      "/tmp/ipykernel_41042/2445998602.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_data[col] = pd.to_numeric(val_data[col], errors='coerce').fillna(0)\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 27590.8242 - mae: 117.5950 - val_loss: 8639.7559 - val_mae: 67.7635 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 14117.2002 - mae: 84.4527 - val_loss: 6469.8628 - val_mae: 57.7515 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 11416.5928 - mae: 75.5525 - val_loss: 4984.8013 - val_mae: 48.2040 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 10527.0293 - mae: 70.2679 - val_loss: 5429.3999 - val_mae: 49.8260 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 10351.3291 - mae: 69.8987 - val_loss: 5207.4155 - val_mae: 49.5431 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 9539.7754 - mae: 67.1430 - val_loss: 4583.6084 - val_mae: 46.1264 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 9153.9023 - mae: 66.3006 - val_loss: 4110.7588 - val_mae: 42.6976 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 8780.5781 - mae: 65.2025 - val_loss: 5248.0176 - val_mae: 48.7752 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 8790.2979 - mae: 64.2052 - val_loss: 5169.2222 - val_mae: 49.4022 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 8874.1592 - mae: 63.6832 - val_loss: 3846.0049 - val_mae: 41.0195 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 8624.3750 - mae: 63.3904 - val_loss: 4085.5576 - val_mae: 42.2332 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 7993.4229 - mae: 61.4672 - val_loss: 3567.6619 - val_mae: 39.4927 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 7979.6338 - mae: 61.7616 - val_loss: 4233.1230 - val_mae: 41.9351 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 7986.0977 - mae: 61.9448 - val_loss: 4014.8237 - val_mae: 42.0570 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 7898.9419 - mae: 61.3938 - val_loss: 3577.0691 - val_mae: 39.4370 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 7718.4536 - mae: 60.2462 - val_loss: 3635.7659 - val_mae: 39.8218 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m890/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8067.7646 - mae: 61.0480\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 8065.5781 - mae: 61.0424 - val_loss: 4314.6499 - val_mae: 43.4456 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 7425.5742 - mae: 59.2034 - val_loss: 4020.2988 - val_mae: 38.9805 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 7449.1567 - mae: 59.6622 - val_loss: 3437.8066 - val_mae: 37.7845 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 6972.0908 - mae: 58.0659 - val_loss: 4091.8330 - val_mae: 41.5918 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 6995.3545 - mae: 58.1182 - val_loss: 3897.6047 - val_mae: 39.8673 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 6738.9785 - mae: 57.5298 - val_loss: 4256.1279 - val_mae: 39.7847 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 7093.5005 - mae: 57.3396 - val_loss: 3937.4648 - val_mae: 39.7380 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m892/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6756.7559 - mae: 57.0130\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 6758.0991 - mae: 57.0166 - val_loss: 3754.9519 - val_mae: 38.5607 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 6579.4282 - mae: 56.5148 - val_loss: 3945.4287 - val_mae: 39.1129 - learning_rate: 2.5000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 6770.4282 - mae: 56.4981 - val_loss: 4085.7734 - val_mae: 41.4695 - learning_rate: 2.5000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 6606.2617 - mae: 56.6367 - val_loss: 3596.6348 - val_mae: 37.5641 - learning_rate: 2.5000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 6600.9717 - mae: 56.0317 - val_loss: 3482.6941 - val_mae: 37.3685 - learning_rate: 2.5000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m894/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6612.7422 - mae: 56.3959\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 6613.8115 - mae: 56.3988 - val_loss: 3773.4614 - val_mae: 39.4304 - learning_rate: 2.5000e-04\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "\u001b[1m291/291\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best R²: 0.7946\n",
      "Validation cost: 1.5954e+07\n",
      "Mean Squared Error (MSE) on validation set: 3.4336e+03\n",
      "Mean Absolute Percentage Error (MAPE): 0.27%\n",
      "Model saved to: /workspaces/Team_Raum-3_BakerySalesPredictions/3_Model/rnn_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not process column Umsatz_last_year - arg must be a list, tuple, 1-d array, or Series\n",
      "\u001b[1m1852/1852\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "Final submission saved to: /workspaces/Team_Raum-3_BakerySalesPredictions/0_DataPreparation/final_submission.csv\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Training\n",
    "##############################################################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import os\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv(\"/workspaces/Team_Raum-3_BakerySalesPredictions/0_DataPreparation/processed_data_imputed.csv\")\n",
    "\n",
    "# Filter out rows with Umsatz = 0\n",
    "data = data[data['Umsatz'] != 0]\n",
    "\n",
    "# Ensure 'Datum' is in datetime format\n",
    "data['Datum'] = pd.to_datetime(data['Datum'], errors='coerce')\n",
    "data = data.sort_values('Datum')\n",
    "\n",
    "# Add time series features\n",
    "data['Umsatz_7day_avg'] = data['Umsatz'].rolling(window=7).mean()\n",
    "data['Umsatz_last_year'] = data['Datum'].apply(lambda x: x - pd.DateOffset(years=1) if pd.notnull(x) else pd.NaT)\n",
    "data = data.merge(data[['Datum', 'Umsatz']], how='left', left_on='Umsatz_last_year', right_on='Datum', suffixes=('', '_last_year'))\n",
    "data.rename(columns={'Umsatz_last_year': 'Umsatz_last_year'}, inplace=True)\n",
    "data.drop(columns=['Datum_last_year'], inplace=True)\n",
    "\n",
    "# Drop rows with NaN values introduced by rolling and shifting\n",
    "data.dropna(subset=['Umsatz_7day_avg', 'Umsatz_last_year'], inplace=True)\n",
    "\n",
    "# Define feature columns\n",
    "feature_columns = [\n",
    "    'KiWo',\n",
    "    'Is_Ferien',\n",
    "    'Holiday',\n",
    "    'Is_Weekend',\n",
    "    'Weihnachten_Sommer',\n",
    "    'Christmas_Sales',\n",
    "    'Temperature_Category',\n",
    "    'Windgeschwindigkeit_Beaufort',\n",
    "    'Rain_Status',\n",
    "    'Cloud_Status',\n",
    "    'Warengruppe_1',\n",
    "    'Warengruppe_2',\n",
    "    'Warengruppe_3',\n",
    "    'Warengruppe_4',\n",
    "    'Warengruppe_5',\n",
    "    'Warengruppe_6',\n",
    "    'Umsatz_7day_avg',\n",
    "    'Umsatz_last_year'\n",
    "]\n",
    "\n",
    "print(\"Final feature columns used in training:\", feature_columns)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "training_start_date = '2013-07-01'\n",
    "training_end_date = '2017-07-31'\n",
    "validation_start_date = '2017-08-01'\n",
    "validation_end_date = '2018-07-31'\n",
    "\n",
    "train_data = data[(data['Datum'] >= training_start_date) & (data['Datum'] <= training_end_date)]\n",
    "val_data = data[(data['Datum'] >= validation_start_date) & (data['Datum'] <= validation_end_date)]\n",
    "\n",
    "# Ensure all features are numeric\n",
    "valid_features = []\n",
    "for col in feature_columns:\n",
    "    if col in train_data.columns and col in val_data.columns:\n",
    "        try:\n",
    "            train_data[col] = pd.to_numeric(train_data[col], errors='coerce').fillna(0)\n",
    "            val_data[col] = pd.to_numeric(val_data[col], errors='coerce').fillna(0)\n",
    "            valid_features.append(col)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process column {col} - {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: Column {col} is missing in train_data or val_data.\")\n",
    "\n",
    "X_train = train_data[valid_features].to_numpy()\n",
    "y_train = train_data['Umsatz'].to_numpy()\n",
    "X_val = val_data[valid_features].to_numpy()\n",
    "y_val = val_data['Umsatz'].to_numpy()\n",
    "\n",
    "# Reshape data for RNN input\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Convert to 3D shape (samples, timesteps, features)\n",
    "def create_sequences(data, target, timesteps=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - timesteps):\n",
    "        X.append(data[i:i+timesteps])\n",
    "        y.append(target[i+timesteps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "timesteps = 7  # Using 7 days of history\n",
    "X_train, y_train = create_sequences(X_train, y_train, timesteps)\n",
    "X_val, y_val = create_sequences(X_val, y_val, timesteps)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(timesteps, X_train.shape[2]), activation='relu', return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32, activation='relu', return_sequences=False, kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Callbacks for early stopping and learning rate adjustment\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "lr_adjustment = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, lr_adjustment],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_val_pred = model.predict(X_val).flatten()\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "validation_cost = mse * len(y_val) / 2\n",
    "\n",
    "print(f\"Best R²: {r2:.4f}\")\n",
    "print(f\"Validation cost: {validation_cost:.4e}\")\n",
    "print(f\"Mean Squared Error (MSE) on validation set: {mse:.4e}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = \"/workspaces/Team_Raum-3_BakerySalesPredictions/3_Model/rnn_model.h5\"\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "\n",
    "##############################################################################\n",
    "# Simulation\n",
    "##############################################################################\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "# Paths to the files\n",
    "processed_data_path = \"/workspaces/Team_Raum-3_BakerySalesPredictions/0_DataPreparation/processed_data_imputed.csv\"\n",
    "sample_submission_path = \"/workspaces/Team_Raum-3_BakerySalesPredictions/0_DataPreparation/sample_submission.csv\"\n",
    "final_submission_path = \"/workspaces/Team_Raum-3_BakerySalesPredictions/0_DataPreparation/final_submission.csv\"\n",
    "model_path = \"/workspaces/Team_Raum-3_BakerySalesPredictions/3_Model/rnn_model.h5\"\n",
    "\n",
    "# Load the processed data and sample submission\n",
    "processed_data = pd.read_csv(processed_data_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "\n",
    "# Dynamically rebuild feature columns\n",
    "feature_columns = [\n",
    "    'KiWo',\n",
    "    'Is_Ferien',\n",
    "    'Holiday',\n",
    "    'Is_Weekend',\n",
    "    'Weihnachten_Sommer',\n",
    "    'Christmas_Sales',\n",
    "    'Temperature_Category',\n",
    "    'Windgeschwindigkeit_Beaufort',\n",
    "    'Rain_Status',\n",
    "    'Cloud_Status',\n",
    "    'Warengruppe_1',\n",
    "    'Warengruppe_2',\n",
    "    'Warengruppe_3',\n",
    "    'Warengruppe_4',\n",
    "    'Warengruppe_5',\n",
    "    'Warengruppe_6',\n",
    "    'Umsatz_7day_avg',\n",
    "    'Umsatz_last_year'\n",
    "]\n",
    "\n",
    "# Add time series features to processed_data\n",
    "processed_data['Datum'] = pd.to_datetime(processed_data['Datum'], errors='coerce')\n",
    "processed_data['Umsatz_7day_avg'] = processed_data['Umsatz'].rolling(window=7).mean()\n",
    "processed_data['Umsatz_last_year'] = processed_data['Datum'].apply(lambda x: x - pd.DateOffset(years=1) if pd.notnull(x) else pd.NaT)\n",
    "processed_data = processed_data.merge(\n",
    "    processed_data[['Datum', 'Umsatz']], how='left', left_on='Umsatz_last_year', right_on='Datum', suffixes=('', '_last_year')\n",
    ")\n",
    "processed_data.rename(columns={'Umsatz_last_year': 'Umsatz_last_year'}, inplace=True)\n",
    "processed_data.drop(columns=['Datum_last_year'], inplace=True)\n",
    "processed_data.dropna(subset=['Umsatz_7day_avg', 'Umsatz_last_year'], inplace=True)\n",
    "\n",
    "# Extract features for prediction\n",
    "valid_features = []\n",
    "for col in feature_columns:\n",
    "    if col in processed_data.columns:\n",
    "        try:\n",
    "            processed_data[col] = pd.to_numeric(processed_data[col], errors='coerce').fillna(0)\n",
    "            valid_features.append(col)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process column {col} - {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: Column {col} is missing in processed_data.\")\n",
    "\n",
    "X_new = processed_data[valid_features].to_numpy()\n",
    "\n",
    "# Normalize features using the same scaler used during training\n",
    "X_new = scaler.transform(X_new)\n",
    "\n",
    "# Convert to 3D shape for RNN input\n",
    "X_new, _ = create_sequences(X_new, np.zeros(X_new.shape[0]), timesteps=timesteps)\n",
    "\n",
    "# Load the trained RNN model\n",
    "model = load_model(\n",
    "    model_path,\n",
    "    custom_objects={'mse': MeanSquaredError()}  # Ensure compatibility with saved model\n",
    ")\n",
    "\n",
    "# Predict the output using the trained RNN model\n",
    "y_pred = model.predict(X_new).flatten()\n",
    "\n",
    "# Add predictions to the processed data DataFrame\n",
    "processed_data['Predicted_Umsatz'] = np.nan\n",
    "processed_data.iloc[-len(y_pred):, processed_data.columns.get_loc('Predicted_Umsatz')] = y_pred\n",
    "\n",
    "# Merge predictions with sample submission to ensure matching structure\n",
    "final_submission = sample_submission[['id']].copy()\n",
    "final_submission = final_submission.merge(\n",
    "    processed_data[['id', 'Predicted_Umsatz']],\n",
    "    how='left',\n",
    "    left_on='id',\n",
    "    right_on='id'\n",
    ")\n",
    "\n",
    "# Rename 'Predicted_Umsatz' to 'Umsatz'\n",
    "final_submission.rename(columns={'Predicted_Umsatz': 'Umsatz'}, inplace=True)\n",
    "\n",
    "# Replace null values in the Umsatz column with 0\n",
    "final_submission['Umsatz'] = final_submission['Umsatz'].fillna(0)\n",
    "\n",
    "# Save the final submission file\n",
    "os.makedirs(os.path.dirname(final_submission_path), exist_ok=True)\n",
    "final_submission.to_csv(final_submission_path, index=False)\n",
    "\n",
    "print(f\"Final submission saved to: {final_submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
