{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../0_DataPreparation/processed_data.csv...\n",
      "Data loaded successfully.\n",
      "Preprocessing data...\n",
      "Defining feature columns...\n",
      "Feature columns: ['KiWo', 'Is_Weekend', 'Temperature_Category', 'Windgeschwindigkeit_Beaufort', 'Rain_Status', 'Bewoelkung', 'Warengruppe_1', 'Warengruppe_2', 'Warengruppe_3', 'Warengruppe_4', 'Warengruppe_5', 'Warengruppe_6']\n",
      "Splitting data into features and target...\n",
      "Splitting data into training and validation sets...\n",
      "Scaling data...\n",
      "Data scaling completed.\n",
      "Setting up the tuner...\n",
      "Reloading Tuner from my_dir/feature_tuning/tuner0.json\n",
      "Tuner setup completed.\n",
      "Defining early stopping...\n",
      "Starting hyperparameter search...\n",
      "Hyperparameter search completed.\n",
      "Retrieving best hyperparameters...\n",
      "Best hyperparameters: {'units_1': 128, 'dropout_1': 0.2, 'units_2': 64, 'dropout_2': 0.4, 'units_3': 32, 'learning_rate': 0.01, 'tuner/epochs': 20, 'tuner/initial_epoch': 7, 'tuner/bracket': 2, 'tuner/round': 2, 'tuner/trial_id': '0012'}\n",
      "Building the model with the best hyperparameters...\n",
      "Building model...\n",
      "Model built successfully.\n",
      "Training the final model...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 17702.4492 - mae: 86.4008 - val_loss: 5390.6748 - val_mae: 43.0218\n",
      "Epoch 2/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6195.9014 - mae: 51.1087 - val_loss: 6144.0825 - val_mae: 44.9810\n",
      "Epoch 3/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6506.6729 - mae: 51.2786 - val_loss: 5286.6963 - val_mae: 43.0374\n",
      "Epoch 4/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 7248.0400 - mae: 51.5577 - val_loss: 6059.5859 - val_mae: 48.5095\n",
      "Epoch 5/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6551.3945 - mae: 51.1162 - val_loss: 5174.7651 - val_mae: 43.8441\n",
      "Epoch 6/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6039.1958 - mae: 50.7330 - val_loss: 5504.8540 - val_mae: 42.7588\n",
      "Epoch 7/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6170.1440 - mae: 50.0970 - val_loss: 4986.2217 - val_mae: 40.9840\n",
      "Epoch 8/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6194.4785 - mae: 50.7273 - val_loss: 6802.2393 - val_mae: 55.3756\n",
      "Epoch 9/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7372.9160 - mae: 53.6959 - val_loss: 5291.2354 - val_mae: 45.8421\n",
      "Epoch 10/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6549.0156 - mae: 51.4918 - val_loss: 5611.6118 - val_mae: 42.7689\n",
      "Epoch 11/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5509.1719 - mae: 48.4699 - val_loss: 5584.4658 - val_mae: 45.4393\n",
      "Epoch 12/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6092.4116 - mae: 48.5709 - val_loss: 5364.9077 - val_mae: 42.3007\n",
      "Final model training completed.\n",
      "Evaluating the model...\n",
      "Validation Loss: 5364.90771484375\n",
      "Validation MAE: 42.30068588256836\n",
      "Computing MAPE...\n",
      "Validation MAPE: 22.90%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras_tuner import Hyperband\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess data\n",
    "# Update the path to point to the correct location of processed_data.csv\n",
    "data_path = \"../0_DataPreparation/processed_data.csv\"  # Ensure this path is correct\n",
    "try:\n",
    "    print(f\"Loading data from {data_path}...\")\n",
    "    data = pd.read_csv(data_path)\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"The file at {data_path} could not be found. Please check the file path.\")\n",
    "\n",
    "# Fill missing values and filter out rows with Umsatz = 0\n",
    "print(\"Preprocessing data...\")\n",
    "data['Bewoelkung'] = data['Bewoelkung'].fillna(data['Bewoelkung'].mean())\n",
    "data = data[data['Umsatz'] != 0]\n",
    "\n",
    "# Define feature columns\n",
    "print(\"Defining feature columns...\")\n",
    "feature_columns = [\n",
    "    'KiWo', 'Is_Weekend', 'Temperature_Category',\n",
    "    'Windgeschwindigkeit_Beaufort', 'Rain_Status',\n",
    "    'Bewoelkung'\n",
    "] + [col for col in data.columns if col.startswith('Warengruppe_')]\n",
    "print(f\"Feature columns: {feature_columns}\")\n",
    "\n",
    "# Split data into features and target\n",
    "print(\"Splitting data into features and target...\")\n",
    "X = data[feature_columns].to_numpy()\n",
    "y = data['Umsatz'].to_numpy()\n",
    "\n",
    "# Split data into training and validation sets\n",
    "print(\"Splitting data into training and validation sets...\")\n",
    "X_train_full, X_val_full, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "print(\"Scaling data...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_full = scaler.fit_transform(X_train_full)\n",
    "X_val_full = scaler.transform(X_val_full)\n",
    "print(\"Data scaling completed.\")\n",
    "\n",
    "# Define a custom model builder\n",
    "def build_model(hp):\n",
    "    print(\"Building model...\")\n",
    "    model = Sequential()\n",
    "    input_shape = X_train_full.shape[1]  # Dynamically get the number of features\n",
    "\n",
    "    # First hidden layer\n",
    "    model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=128, step=32), \n",
    "                    activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Second hidden layer\n",
    "    model.add(Dense(units=hp.Int('units_2', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Third hidden layer\n",
    "    model.add(Dense(units=hp.Int('units_3', min_value=16, max_value=64, step=16), activation='relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    print(\"Model built successfully.\")\n",
    "    return model\n",
    "\n",
    "# Set up the tuner\n",
    "print(\"Setting up the tuner...\")\n",
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_mae',\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory='my_dir',\n",
    "    project_name='feature_tuning'\n",
    ")\n",
    "print(\"Tuner setup completed.\")\n",
    "\n",
    "# Early stopping\n",
    "print(\"Defining early stopping...\")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Run the hyperparameter search\n",
    "print(\"Starting hyperparameter search...\")\n",
    "tuner.search(\n",
    "    X_train_full, y_train,\n",
    "    validation_data=(X_val_full, y_val),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Hyperparameter search completed.\")\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "print(\"Retrieving best hyperparameters...\")\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best hyperparameters: {best_hps.values}\")\n",
    "\n",
    "# Build the model with the best hyperparameters\n",
    "print(\"Building the model with the best hyperparameters...\")\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the final model\n",
    "print(\"Training the final model...\")\n",
    "history = model.fit(\n",
    "    X_train_full, y_train,\n",
    "    validation_data=(X_val_full, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Final model training completed.\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating the model...\")\n",
    "val_loss, val_mae = model.evaluate(X_val_full, y_val, verbose=0)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "# Compute Mean Absolute Percentage Error (MAPE)\n",
    "print(\"Computing MAPE...\")\n",
    "y_val_pred = model.predict(X_val_full, verbose=0)\n",
    "mape = 100 * np.mean(np.abs((y_val - y_val_pred.flatten()) / y_val))\n",
    "print(f\"Validation MAPE: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/439\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 788us/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step\n",
      "Final submission saved to: /workspaces/Team_Raum-3_BakerySalesPredictions/0_DataPreparation/final_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Paths to the files\n",
    "processed_data_path = \"/workspaces/Team_Raum-3_BakerySalesPredictions/0_DataPreparation/processed_data.csv\"\n",
    "sample_submission_path = \"/workspaces/Team_Raum-3_BakerySalesPredictions/0_DataPreparation/sample_submission.csv\"\n",
    "final_submission_path = \"/workspaces/Team_Raum-3_BakerySalesPredictions/0_DataPreparation/final_submission.csv\"\n",
    "model_path = \"/workspaces/Team_Raum-3_BakerySalesPredictions/3_Model/nn_model.h5\"\n",
    "\n",
    "# Load the processed data and sample submission\n",
    "processed_data = pd.read_csv(processed_data_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "\n",
    "# Dynamically rebuild feature columns\n",
    "feature_columns = [\n",
    "    'KiWo', 'Is_Weekend', 'Temperature_Category',\n",
    "    'Windgeschwindigkeit_Beaufort', 'Rain_Status',\n",
    "    'Bewoelkung', 'Warengruppe_1', 'Warengruppe_2',\n",
    "    'Warengruppe_3', 'Warengruppe_4', 'Warengruppe_5', 'Warengruppe_6'\n",
    "]\n",
    "\n",
    "# Extract features for prediction\n",
    "X_new = processed_data[feature_columns].apply(pd.to_numeric, errors='coerce').fillna(0).to_numpy(dtype=np.float64)\n",
    "\n",
    "# Load the trained neural network model\n",
    "model = load_model(\n",
    "    model_path,\n",
    "    custom_objects={'mse': MeanSquaredError()}  # Ensure compatibility with saved model\n",
    ")\n",
    "\n",
    "# Normalize features using the same scaler used during training\n",
    "scaler = StandardScaler()\n",
    "X_new = scaler.fit_transform(X_new)  # Use saved scaler if available\n",
    "\n",
    "# Predict the output using the trained neural network\n",
    "y_pred = model.predict(X_new).flatten()\n",
    "\n",
    "# Add predictions to the processed data DataFrame\n",
    "processed_data['Predicted_Umsatz'] = y_pred\n",
    "\n",
    "# Merge predictions with sample submission to ensure matching structure\n",
    "final_submission = sample_submission[['id']].copy()\n",
    "final_submission = final_submission.merge(\n",
    "    processed_data[['ID', 'Predicted_Umsatz']],\n",
    "    how='left',\n",
    "    left_on='id',\n",
    "    right_on='ID'\n",
    ")\n",
    "\n",
    "# Drop the redundant 'ID' column and rename 'Predicted_Umsatz' to 'Umsatz'\n",
    "final_submission.drop(columns=['ID'], inplace=True)\n",
    "final_submission.rename(columns={'Predicted_Umsatz': 'Umsatz'}, inplace=True)\n",
    "\n",
    "# Replace null values in the Umsatz column with 0\n",
    "final_submission['Umsatz'] = final_submission['Umsatz'].fillna(0)\n",
    "\n",
    "# Save the final submission file\n",
    "os.makedirs(os.path.dirname(final_submission_path), exist_ok=True)\n",
    "final_submission.to_csv(final_submission_path, index=False)\n",
    "\n",
    "print(f\"Final submission saved to: {final_submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
